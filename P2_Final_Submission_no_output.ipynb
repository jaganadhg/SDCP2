{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Recognizing traffic signs from real-time video streams from smart vehicles is one of the demanding solutions in today's automobile industry. It became a demanding solution in autonomous vehicles, driver assistance systems and mobile mapping. The Traffic Sign Recognition (TSR) challenge consists two components 1) traffic sign detection and traffic sign classification. Traffic sign detection is the activity of accurate localizing the traffic sign in an image, while traffic sign classification assigns a label (symbol name/details) to the localized picture. Recent developments in Deep Learning and publically available data sets such as Belgium and German Traffic Sign Data accelerated many innovations in this field. The objective of the current project is to build a traffic sign classification system with the German Traffic Signs dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "The dataset is supplied as Python pickled objects. The training, validation and test data are in the same format. Each picked object contains the following information: 'labels', 'coords', 'sizes', 'features'. The 'features' contain the image pixel data, 'coords' contains localization information, 'size' contains image size and the  'labels' contains a numeric value representing the traffic sign represented in the image. The train, test and valid data consist 34799, 12630, 4410 samples respectively. There are 43 traffic signs in this image data sets, and it translates to 43 labels in the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sn\n",
    "plt.rcParams['figure.figsize'] = (9,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import exposure\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_files = glob.glob(\"data/*.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = None\n",
    "test = None\n",
    "valid = None\n",
    "\n",
    "for fname in data_files:\n",
    "    with open(fname,mode='rb') as pf:\n",
    "        if fname.endswith(\"test.p\"):\n",
    "            test = pickle.load(pf)\n",
    "        elif fname.endswith(\"train.p\"):\n",
    "            train = pickle.load(pf)\n",
    "        elif fname.endswith(\"valid.p\"):\n",
    "            valid = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total records in Train Test and Valid Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train[\"features\"]),len(test[\"features\"]),len(valid[\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution Analysis in the Data\n",
    "An analysis of the class distribution in training data suggests that it is a highly imbalanced dataset. The well-represented class consists samples in the range of 1000 to 2500 and the under-represented classed consists instances in the range of 120 to 850. In general Machine Learning scenarios, the under-represented or imbalanced class distribution is not an ideal situation to start with any Machine Learning experiment. So it is recommended to perform either additional data collection or perform an oversampling or undersampling of the data. Oversampling of data is a typical strategy for the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.Series(train['labels'])\n",
    "train_label_count = train_df.value_counts()\n",
    "test_df = pd.Series(test['labels'])\n",
    "test_label_count = test_df.value_counts()\n",
    "valid_df = pd.Series(valid['labels'])\n",
    "valid_label_count = valid_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label_count_df = pd.DataFrame()\n",
    "train_label_count_df[\"ClassId\"] = train_label_count.index\n",
    "train_label_count_df[\"Count\"] = train_label_count.values\n",
    "label_map = pd.read_csv(\"signnames.csv\")\n",
    "train_joined_labels = pd.merge(train_label_count_df,label_map,on='ClassId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sign_label_map = dict(zip(train_joined_labels.ClassId,train_joined_labels.SignName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sign_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_samples =[np.where(train[\"labels\"]==i)[0][0] for i, x in enumerate(np.unique(train[\"labels\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [train[\"features\"][img_indx] for indx,img_indx in enumerate(image_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_one_sample(samples):\n",
    "    fig = plt.figure(figsize=(9, 9))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "    for i in range(len(samples)):\n",
    "        ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(samples[i].squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sample Image from all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_one_sample(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_class_dict(train_label_count,color):\n",
    "\n",
    "    train_label_count_df = pd.DataFrame()\n",
    "    train_label_count_df[\"ClassId\"] = train_label_count.index\n",
    "    train_label_count_df[\"Count\"] = train_label_count.values\n",
    "    label_map = pd.read_csv(\"signnames.csv\")\n",
    "    train_joined_labels = pd.merge(train_label_count_df,label_map,on='ClassId')\n",
    "    train_joined_labels.index = train_joined_labels.ClassId\n",
    "    sbp = sn.barplot(x=\"SignName\",y=\"Count\",data=train_joined_labels,color=color)\n",
    "    d = plt.setp(sbp.get_xticklabels(), rotation=90)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training Data Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_class_dict(train_label_count,\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test Data Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_class_dict(test_label_count,\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Validation Data Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_class_dict(valid_label_count,\"m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Synthetic Data Generation\n",
    "The additional data generation to overcome the imbalanced data is performed with Keras library. Keras provided image augmentation API with a comprehensive set of parameters to satisfy different experiment purposes. The settings used in the current projects includes\n",
    "* featurewise_center: Set input mean to 0 over the dataset, feature-wise.\n",
    "* rotation_range:  Degree range for random rotations. The rotation range was specified as 17 for the augmentation.\n",
    "* width_shift_range: Range for random horizontal shifts. Specified value 0.1\n",
    "*  height_shift_range: Range for random vertical shifts. Specified value 0.1\n",
    "* shear_range:  Shear Intensity (Shear angle in the counter-clockwise direction as radians). Specified Value: 0.3\n",
    "* zoom_range: Range for random zoom. Specified Value: 0.15\n",
    "* horizontal_flip:  Randomly flip inputs horizontally. Not performed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "augumentar = ImageDataGenerator(rotation_range=17,width_shift_range=0.1,\\\n",
    "height_shift_range=0.1,shear_range=0.3,zoom_range=0.15,horizontal_flip=False,\\\n",
    "fill_mode='nearest',featurewise_center=True)\n",
    "#fill_mode='nearest',featurewise_center=True,zca_whitening=True)\n",
    "\n",
    "def image_aug(X,y):\n",
    "    \"\"\"\n",
    "    Apply image Augumentation with Keras\n",
    "    \"\"\"\n",
    "    X_aug = None\n",
    "    y_aug = None\n",
    "    #augumentar.fit(X)\n",
    "    for X_batch, y_batch in augumentar.flow(X, y, batch_size=X.shape[0], shuffle=False):\n",
    "        X_aug = X_batch.astype('uint8')\n",
    "        y_aug = y_batch\n",
    "        break\n",
    "    return (X_aug,y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "augm_images = image_aug(train[\"features\"],train[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A Sample Augumented Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(augm_images[0][1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_augumented = augm_images[0]\n",
    "Y_train_augumented = augm_images[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Augumented Image sample from 43 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aug_samplees = [X_train_augumented[img_indx] for indx,img_indx in enumerate(image_samples)]\n",
    "plot_one_sample(aug_samplees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Motion Blur \n",
    "\n",
    "The additional image data generation step adopted in this project is Motion Blur. Motion blur is the apparent streaking of rapidly moving objects in a still image. It helps the traffic sign classifier to almost accurately classify a localized sign to the proper category. In real-time image processing, this techniques is a very useful approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mblur(X):\n",
    "    \"\"\"\n",
    "    Create a motion blur\n",
    "    \"\"\"\n",
    "    X_out = np.empty((X.shape)).astype('uint8')\n",
    "    size = 4\n",
    "    kernel_motion_blur = np.zeros((size, size))\n",
    "    kernel_motion_blur[int((size-1)/2), :] = np.ones(size)\n",
    "    kernel_motion_blur = kernel_motion_blur / size\n",
    "    for idx, img in enumerate(X):\n",
    "        X_out[idx] = cv2.filter2D(img, -1, kernel_motion_blur)\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_motion_blur = mblur(train[\"features\"])\n",
    "y_train_motion_blur = train[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sample Results of Motion Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_samplees = [X_train_motion_blur[img_indx] for indx,img_indx in enumerate(image_samples)]\n",
    "\n",
    "plot_one_sample(mb_samplees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing \n",
    "In this project, we preprocessed the images before it is being presented to the algorithm for model building. The preprocessing steps adopted in the projects are normalizing the exposure and converting the images to grayscale. This steps will help us preventing some of the adversarial scenarios, such as negative images are not getting classified correctly. Recent researched in the images processing, and Deep Learning reveals that negatives images will be a challenging one for many models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    img_c = cv2.cvtColor(img, (cv2.COLOR_BGR2YUV))[:,:,0]\n",
    "    img_c = (img_c / 255.).astype(np.float32)\n",
    "    img_c = (exposure.equalize_adapthist(img_c,) - 0.5)\n",
    "    img_c = img_c.reshape(img_c.shape + (1,))\n",
    "    return img_c\n",
    "\n",
    "def preprocess_data(X):\n",
    "    X_processed = np.empty((X.shape[0],X.shape[1],X.shape[2],1)).astype(np.float32) \n",
    "    for idx, img in enumerate(X):\n",
    "        X_processed[idx] = process_image(img)\n",
    "    return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_raw_p = preprocess_data(train[\"features\"])\n",
    "y_train_raw_p = train[\"labels\"]\n",
    "X_train_aug_p = preprocess_data(X_train_augumented)\n",
    "y_train_aug_p = Y_train_augumented\n",
    "X_train_mb_p = preprocess_data(X_train_motion_blur)\n",
    "y_train_mb_p = y_train_motion_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_raw_p = preprocess_data(test[\"features\"])\n",
    "y_test_raw_p = test[\"labels\"]\n",
    "\n",
    "X_valid_raw_p = preprocess_data(valid[\"features\"])\n",
    "y_valid_raw_p = valid[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Preprocessed Raw Training Data Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_samplees = [X_train_raw_p[img_indx] for indx,img_indx in enumerate(image_samples)]\n",
    "\n",
    "plot_one_sample(mb_samplees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessed Augumented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_samplees = [X_train_aug_p[img_indx] for indx,img_indx in enumerate(image_samples)]\n",
    "\n",
    "plot_one_sample(mb_samplees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_raw_p,X_train_aug_p,X_train_mb_p), axis=0)\n",
    "y_train = np.concatenate((y_train_raw_p,y_train_aug_p,y_train_mb_p), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "The current model is inspired by the LeNet and names as \"ThirdEye.\" The Architecture has two convolution layer followed by a fully connected layer. RELU is used as activation function in all the three layers. Drop-out rate of 20,30 and 40 is specified in respective layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_classes = len(np.unique(train[\"labels\"]))\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "apply_dropout = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def third_eye_net(X, w, b, dropout):\n",
    "    #third_eye_net(x, weights, biases, apply_dropout):\n",
    "    if dropout is not None:\n",
    "        print (\"Training\")\n",
    "    else:\n",
    "        print (\"Evalutation\") \n",
    "        \n",
    "    layer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Layer 1: Convolutional. \n",
    "    Input = 32x32x1. \n",
    "    Output = 28x28x12.\n",
    "    \"\"\"\n",
    "    conv1   = tf.nn.conv2d(X, w[layer], strides=[1, 1, 1, 1], padding='VALID') + b[layer]\n",
    "    layer += 1\n",
    "\n",
    "    \"\"\"\n",
    "    Activation.\n",
    "    \"\"\"\n",
    "    conv1 = tf.nn.relu(conv1, name = 'eye1')\n",
    "\n",
    "    \"\"\"\n",
    "    # Pooling. \n",
    "    Input = 28x28x12. \n",
    "    Output = 14x14x12.\n",
    "    \"\"\"\n",
    "    \n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    \"\"\"\n",
    "    Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    conv1 = tf.cond(apply_dropout, lambda: tf.nn.dropout(conv1, keep_prob = 0.8), lambda: conv1)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 2: Convolutional. \n",
    "    Output = 10x10x24.\n",
    "    \"\"\"\n",
    "    conv2   = tf.nn.conv2d(conv1, w[layer], strides=[1, 1, 1, 1], padding='VALID') +  b[layer]\n",
    "    layer += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation.\n",
    "    \"\"\"\n",
    "    conv2 = tf.nn.relu(conv2, name = 'eye2')\n",
    "\n",
    "    \"\"\"\n",
    "    Pooling. \n",
    "    Input = 10x10x24. \n",
    "    Output = 5x5x24.\n",
    "    \"\"\"\n",
    "    \n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    \"\"\"\n",
    "    Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    conv2 = tf.cond(apply_dropout, lambda: tf.nn.dropout(conv2, keep_prob = 0.7), lambda: conv2)\n",
    "\n",
    "    \"\"\"\n",
    "    Input = 14x14x12. \n",
    "    Output = 7x7x12 = 588\n",
    "    \"\"\"\n",
    "    conv1_1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    shape = conv1_1.get_shape().as_list()\n",
    "    conv1_1 = tf.reshape(conv1_1, [-1, shape[1] * shape[2] * shape[3]])    \n",
    "    \n",
    "    \"\"\"\n",
    "    Flatten conv2 Input = 5x5x24. Output = 600\n",
    "    \"\"\"\n",
    "    shape = conv2.get_shape().as_list()\n",
    "    conv2 = tf.reshape(conv2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    fc0 = tf.concat([conv1_1, conv2],1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Layer 3: Fully Connected. Input = 588+600 = 1188. Output = 320.\n",
    "    \"\"\"\n",
    "    \n",
    "    fc1   = tf.matmul(fc0, w[layer]) + b[layer]\n",
    "    layer += 1\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation.\n",
    "    \"\"\"\n",
    "    \n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    fc1 = tf.cond(dropout, lambda: tf.nn.dropout(fc1, keep_prob = 0.6), lambda: fc1)\n",
    "    logits = tf.matmul(fc1, w[layer]) + b[layer]\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "beta = 0.001\n",
    "\n",
    "\n",
    "weights = [\n",
    "    tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 12), mean = mu, stddev = sigma)),\n",
    "    tf.Variable(tf.truncated_normal(shape=(5, 5, 12, 24), mean = mu, stddev = sigma)),\n",
    "    tf.Variable(tf.truncated_normal(shape=(1188, 320), mean = mu, stddev = sigma)),\n",
    "    tf.Variable(tf.truncated_normal(shape=(320, n_classes), mean = mu, stddev = sigma))\n",
    "]\n",
    "biases = [\n",
    "   tf.Variable(tf.zeros(12)),\n",
    "   tf.Variable(tf.zeros(24)),\n",
    "   tf.Variable(tf.zeros(320)),\n",
    "   tf.Variable(tf.zeros(n_classes))\n",
    "]\n",
    "\n",
    "\n",
    "logits = third_eye_net(x, weights, biases, apply_dropout)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "regularizer = tf.reduce_sum([tf.nn.l2_loss(w) for w in weights])\n",
    "loss = tf.reduce_mean(loss_operation + beta * regularizer)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss)\n",
    "#tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, apply_dropout: False})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_validation_accuracy = 0.0\n",
    "train_accuracy = list()\n",
    "valid_accuracy = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, apply_dropout: True })\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid_raw_p, y_valid_raw_p)\n",
    "        valid_accuracy.append(validation_accuracy)\n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "        train_accuracy.append(training_accuracy)\n",
    "        if (validation_accuracy > best_validation_accuracy):\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            saver.save(sess, './tenet')\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Perfromance in Validation and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "px = list(range(1,11))\n",
    "plt.plot(px,train_accuracy,'-g',label=\"Accuracy in Training Data\")\n",
    "plt.plot(px,valid_accuracy,'-r',label=\"Accuracy in Validation Data\")\n",
    "plt.xlabel('Epochs', fontsize=13)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.legend()\n",
    "plt.xlim(1,10)\n",
    "plt.ylim(0.8,1.0)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Results\n",
    "During the training phase, the model performance was very satisfactory and resulted in about 97% accuracy in the validation data. The evaluation results in training, validation and test data are: \n",
    "* Training Accuracy = 0.972\n",
    "* Validation Accuracy = 0.978\n",
    "* Test Accuracy = 0.965\n",
    "\n",
    "The evaluation results in Test Data is \n",
    "* test accuracy: 0.964845605625\n",
    "* Precision 0.95536102367\n",
    "* Recall 0.964845605701\n",
    "* f1_score 0.964505652242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    saver.restore(sess, './tenet')\n",
    "\n",
    "    training_accuracy = evaluate(X_train, y_train)\n",
    "    print(\"Training Accuracy = {:.3f}\".format(training_accuracy))\n",
    "    validation_accuracy = evaluate(X_valid_raw_p, y_valid_raw_p)\n",
    "    print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "\n",
    "    test_accuracy = evaluate(X_test_raw_p, y_test_raw_p)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    \n",
    "    #metrics\n",
    "    y_p = tf.argmax(logits, 1)\n",
    "    y_pred = sess.run( y_p, feed_dict={x: X_test_raw_p, y: y_test_raw_p, apply_dropout: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "     \n",
    "print (\"test accuracy:\", test_accuracy)\n",
    "y_true = y_test_raw_p\n",
    "print (\"Precision\", metrics.precision_score(y_true, y_pred, average='macro'))\n",
    "print (\"Recall\", metrics.recall_score(y_true, y_pred, average='micro'))\n",
    "print (\"f1_score\", metrics.f1_score(y_true, y_pred, average='weighted'))\n",
    "print (\"Confusion_matrix\")\n",
    "cm = metrics.confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Results - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(cm,annot=False,annot_kws={\"size\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model on New Images\n",
    "A set of 38 random images of traffic signs were extracted from various web resources and used for the model evaluation. In this data, the model showed a performance of 71% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test_ext = None\n",
    "with open(\"data/gd_new_test.p\",\"rb\") as testf:\n",
    "    x_test_ext = pickle.load(testf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_custom = np.array([21,39,17,17,17,39,39, 40,40,34,28,39,0,17,38,\\\n",
    "13,40,13,38,38,11,0,28,0, 99, 99, 99, 32, 40,28, 40,40,28,24, 0, 0, \\\n",
    "0,0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_test_ext[\"test_new\"][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new_test = preprocess_data(np.array(x_test_ext[\"test_new\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(X_new_test[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './tenet')\n",
    "    \n",
    "    test_accuracy = evaluate(X_new_test, y_custom)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sign_label_map\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "feed_dict_new = feed_dict={x: X_new_test, y: y_custom, apply_dropout: False}\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './tenet')\n",
    "    predictions = sess.run(logits,feed_dict = feed_dict_new)\n",
    "    top5_pred = sess.run([logits, tf.nn.top_k(logits, 5)], feed_dict=feed_dict_new)\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(6):\n",
    "    plt.figure(figsize = (6,3))\n",
    "    gs = gridspec.GridSpec(1, 3)\n",
    "    plt.subplot(gs[0])\n",
    "    plt.axis('off')\n",
    "    plt.imshow((x_test_ext[\"test_new\"][i].squeeze()))\n",
    "    plt.subplot(gs[1])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(gs[2])\n",
    "    plt.barh(6-np.arange(5),top5_pred[1][0][i], align='center')\n",
    "    for i_label in range(5):\n",
    "        plt.text(top5_pred[1][0][i][i_label]+.02,6-i_label-.15,\n",
    "            sign_label_map[top5_pred[1][1][i][i_label]])\n",
    "    plt.axis('off');\n",
    "\n",
    "    plt.show();    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    \n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input, apply_dropout: False})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) \n",
    "        plt.title('FeatureMap ' + str(featuremap))\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", \\\n",
    "            vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", \\\n",
    "            vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", \\\n",
    "            vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", \\\n",
    "            cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Neural Network's State with Test Images\n",
    "The code for the same is adapted from Udacity's training materials and the sample notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './tenet')\n",
    "    act1 = tf.get_default_graph().get_tensor_by_name(\"eye1:0\")\n",
    "    outputFeatureMap(X_new_test, act1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './tenet')\n",
    "    act1 = tf.get_default_graph().get_tensor_by_name(\"eye2:0\")\n",
    "    outputFeatureMap(X_new_test, act1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Step : Softmax Probabilities in Random Images Picked from Belgium Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "belgium_data = None\n",
    "with open(\"data/belg_data.p\",\"rb\") as testf:\n",
    "    belgium_data = pickle.load(testf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_belgium = belgium_data[\"belg_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_belg_pp = preprocess_data(np.array(X_belgium))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "feed_dict_new = feed_dict={x: X_belg_pp, apply_dropout: False}\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './tenet')\n",
    "    predictions = sess.run(logits,feed_dict = feed_dict_new)\n",
    "    top5_pred = sess.run([logits, tf.nn.top_k(logits, 5)], feed_dict=feed_dict_new)\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(10):\n",
    "    #plt.figure(figsize = (10,3))\n",
    "    #plt.figure(figsize=(9, 9))\n",
    "    plt.figure(figsize=(3,3))\n",
    "    gs = gridspec.GridSpec(1, 3)\n",
    "    plt.subplot(gs[0])\n",
    "    plt.axis('off')\n",
    "    plt.imshow((X_belgium[i].squeeze()))\n",
    "    plt.subplot(gs[1])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(gs[2])\n",
    "    plt.barh(6-np.arange(5),top5_pred[1][0][i], align='center')\n",
    "    for i_label in range(5):\n",
    "        plt.text(top5_pred[1][0][i][i_label]+.02,6-i_label-.15,\n",
    "            sign_label_map[top5_pred[1][1][i][i_label]])\n",
    "    plt.axis('off');\n",
    "\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
